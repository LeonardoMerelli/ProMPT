{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages.**"
      ],
      "metadata": {
        "id": "ZxX6hGEXxmKL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeRZ2c1rHT0X"
      },
      "outputs": [],
      "source": [
        "!pip install spacy pandas rapidfuzz newspaper3k lxml_html_clean validators\n",
        "# Download model.\n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount to Google drive.**"
      ],
      "metadata": {
        "id": "WudUY3rKxhNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to google drive.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Define the base path for your Google Drive.\n",
        "base_path = '/content/drive'\n",
        "\n",
        "# Define the specific folder path within your Google Drive.\n",
        "folder_path = 'MyDrive/Colab Notebooks/Crafting Tech'\n",
        "\n",
        "# Combine the base path and folder path to create the full mount path.\n",
        "full_project_path = os.path.join(base_path, folder_path)\n",
        "\n",
        "# Mount your drive.\n",
        "drive.mount(base_path, force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGWqhcHSrtyw",
        "outputId": "1af3b998-952c-45d6-82ef-839e32081899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taxonomies**"
      ],
      "metadata": {
        "id": "hHrtS4OgYK4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define taxonomy for post-war problems.\n",
        "taxonomy = {\n",
        "    \"Post-War Problems\": {\n",
        "        \"Security\": [\"landmines\", \"militia violence\", \"arms trafficking\", \"civil unrest\", \"violence\"],\n",
        "        \"Governance\": [\"corruption\", \"lack of elections\", \"political instability\", \"power vacuum\"],\n",
        "        \"Economy\": [\"unemployment\", \"collapsed economy\", \"poverty\", \"inflation\"],\n",
        "        \"Health\": [\"trauma\", \"disease outbreaks\", \"mental health\", \"lack of hospitals\", \"malnutrition\"],\n",
        "        \"Displacement\": [\"refugees\", \"internally displaced persons\", \"repatriation\", \"migration\"],\n",
        "        \"Infrastructure\": [\"destroyed infrastructure\", \"lack of electricity\", \"damaged roads\", \"communication breakdown\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define taxonomy for conflict types.\n",
        "conflict_types = [\n",
        "    \"civil war\", \"ethnic conflict\", \"religious conflict\", \"invasion\", \"border war\",\n",
        "    \"insurgency\", \"terrorism\", \"revolution\", \"proxy war\", \"occupation\"\n",
        "]"
      ],
      "metadata": {
        "id": "biKNx1ZFYMdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions.**"
      ],
      "metadata": {
        "id": "EGIXj4SIxq99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP, text extraction, and matching\n",
        "import spacy\n",
        "from newspaper import Article\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "# Data handling and utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, re, json, datetime\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "import validators\n",
        "from dateutil import parser as date_parser\n",
        "\n",
        "# Load the SpaCy transformer model with GPU acceleration for faster processing.\n",
        "spacy.require_gpu()\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# Extract article text from a URL\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads and extracts main article text from a given URL using newspaper3k.\n",
        "    Returns an empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    article = Article(url)\n",
        "    try:\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to extract from URL: {url}\\nReason: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def ensure_file_exists(filename):\n",
        "    \"\"\"\n",
        "    Creates the file if it doesn't exist.\n",
        "    Adds CSV headers if it's a .csv file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            print(f\"✅ Created file: {filename}\")\n",
        "            # You can optionally add headers or initial content here if needed.\n",
        "            if filename.endswith(\".csv\"):\n",
        "              # Generates the first line of the .csv-file.\n",
        "              f.write(\"region,actors,problem,description,timeline,conflict_type,link\\n\")\n",
        "\n",
        "def extractActors(doc, percentile=90, score_cutoff=85, top_n=5):\n",
        "    \"\"\"\n",
        "    Extracts a list of prominent actors (ORG, PERSON, NORP) from the document.\n",
        "    Uses frequency thresholds and fuzzy matching to identify key entities.\n",
        "    \"\"\"\n",
        "    actors = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\", \"NORP\"]]\n",
        "    actor_frequencies = Counter(actors)\n",
        "\n",
        "    if not actor_frequencies:\n",
        "        return [\"Unknown\"]\n",
        "\n",
        "    # Adaptive threshold.\n",
        "    freq_values = list(actor_frequencies.values())\n",
        "    threshold = max(1, int(np.percentile(freq_values, percentile)))\n",
        "\n",
        "    frequent_actors = [r for r in actor_frequencies if actor_frequencies[r] >= threshold]\n",
        "\n",
        "    # Fuzzy match & rank.\n",
        "    matched = []\n",
        "    for actor in frequent_actors:\n",
        "        match = process.extractOne(actor, actors, scorer=fuzz.partial_ratio, score_cutoff=score_cutoff)\n",
        "        if match:\n",
        "            matched.append((match[0], match[1]))\n",
        "\n",
        "    matched.sort(key=lambda x: (x[1], actor_frequencies[x[0]]), reverse=True)\n",
        "\n",
        "    # Select top_n unique actors.\n",
        "    selected_actors = []\n",
        "    seen_actors = set()  # Keep track of seen actors to avoid duplicates.\n",
        "    for actor, score in matched:\n",
        "        if actor not in seen_actors:\n",
        "            selected_actors.append(actor)\n",
        "            seen_actors.add(actor)\n",
        "            if len(selected_actors) == top_n:\n",
        "                break\n",
        "\n",
        "    return selected_actors if selected_actors else [\"Unknown\"]\n",
        "\n",
        "def extractRegion(doc, percentile=90, score_cutoff=85, top_n=5):\n",
        "    \"\"\"\n",
        "    Extracts regions (GPE, LOC) using frequency and fuzzy filtering.\n",
        "    Returns top_n most relevant geographical entities.\n",
        "    \"\"\"\n",
        "    regions = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
        "    region_frequencies = Counter(regions)\n",
        "\n",
        "    if not region_frequencies:\n",
        "        return [\"Unknown\"]\n",
        "\n",
        "    # Adaptive threshold.\n",
        "    freq_values = list(region_frequencies.values())\n",
        "    threshold = max(1, int(np.percentile(freq_values, percentile)))\n",
        "\n",
        "    frequent_regions = [r for r in region_frequencies if region_frequencies[r] >= threshold]\n",
        "\n",
        "    # Fuzzy match & rank.\n",
        "    matched = []\n",
        "    for region in frequent_regions:\n",
        "        match = process.extractOne(region, regions, scorer=fuzz.partial_ratio, score_cutoff=score_cutoff)\n",
        "        if match:\n",
        "            matched.append((match[0], match[1]))\n",
        "\n",
        "    matched.sort(key=lambda x: (x[1], region_frequencies[x[0]]), reverse=True)\n",
        "\n",
        "    # Select top_n unique regions.\n",
        "    selected_regions = []\n",
        "    seen_regions = set()  # Keep track of seen regions to avoid duplicates.\n",
        "    for region, score in matched:\n",
        "        if region not in seen_regions:\n",
        "            selected_regions.append(region)\n",
        "            seen_regions.add(region)\n",
        "            if len(selected_regions) == top_n:\n",
        "                break\n",
        "\n",
        "    return selected_regions if selected_regions else [\"Unknown\"]\n",
        "\n",
        "\n",
        "def extractTimeline(doc):\n",
        "    \"\"\"\n",
        "    Extracts the most relevant date associated with post-war problems.\n",
        "    Prioritizes dates with higher specificity and relevance to problem keywords.\n",
        "    \"\"\"\n",
        "    problem_keywords = [kw.lower() for sublist in taxonomy[\"Post-War Problems\"].values() for kw in sublist]\n",
        "    date_candidates = []\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ != \"DATE\":\n",
        "            continue\n",
        "\n",
        "        date_text = ent.text.strip()\n",
        "        context_window = doc[max(ent.start - 5, 0): ent.end + 5].text.lower()\n",
        "\n",
        "        # Only consider if the date includes a year.\n",
        "        if not re.search(r\"\\d{4}\", date_text):\n",
        "            continue\n",
        "\n",
        "        # Check if context contains any relevant problem keyword.\n",
        "        if any(kw in context_window for kw in problem_keywords):\n",
        "            try:\n",
        "                parsed_date = date_parser.parse(date_text, fuzzy=True)\n",
        "                iso_date = parsed_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "                # Optionally: filter out dates before 1990.\n",
        "                if parsed_date.year >= 1990:\n",
        "                    specificity_score = len(re.findall(r\"\\d\", date_text))\n",
        "                    date_candidates.append((iso_date, specificity_score))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    if not date_candidates:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Sort by specificity (more digits = better date).\n",
        "    date_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return date_candidates[0][0]\n",
        "\n",
        "def extractConflictType(doc, upper_cutoff=95, lower_cutoff=70, step=5, window_size=3):\n",
        "    \"\"\"\n",
        "    Matches document content to predefined conflict types using fuzzy matching.\n",
        "    Searches over sentence windows, relaxing thresholds until a match is found.\n",
        "    \"\"\"\n",
        "    type_scores = []\n",
        "    current_cutoff = upper_cutoff\n",
        "    # Filter on the cutoff frequency of occurence.\n",
        "    while current_cutoff >= lower_cutoff and not type_scores:\n",
        "        for i in range(0, len(list(doc.sents)), window_size):\n",
        "            window_text = \" \".join([sent.text.lower() for sent in list(doc.sents)[i:i + window_size]])\n",
        "            result = process.extractOne(window_text, conflict_types, scorer=fuzz.partial_ratio, score_cutoff=current_cutoff)\n",
        "            if result:\n",
        "                type_scores.append(result[0])\n",
        "\n",
        "        # If no match is found, lower the cutoff value.\n",
        "        current_cutoff -= step\n",
        "\n",
        "    if not type_scores:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Return the most frequent conflict type found.\n",
        "    most_common = Counter(type_scores).most_common(1)\n",
        "    return most_common[0][0]\n",
        "\n",
        "def extractProblems(doc, taxonomy, regions, actors, timeline, conflict_type , score_cutoff=85):\n",
        "    \"\"\"\n",
        "    Identifies and extracts sentences containing post-war problems using keyword matching.\n",
        "    Each match is combined with contextual information for structured output.\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "    for sent in doc.sents:\n",
        "        sentence_text = sent.text.lower()\n",
        "        for category, keywords in taxonomy[\"Post-War Problems\"].items():\n",
        "            # Optimized keyword matching with process.extractOne\n",
        "            result = process.extractOne(\n",
        "                sentence_text, keywords, scorer=fuzz.partial_ratio, score_cutoff=score_cutoff)\n",
        "            if result:\n",
        "                matched_keyword = result[0] if isinstance(result[0], str) else result[0][0]\n",
        "            else:\n",
        "                matched_keyword = None  # Or some other default value.\n",
        "            # If we find a match we add it to the extracted_data list.\n",
        "            if matched_keyword:\n",
        "                extracted_data.append({\n",
        "                    \"regions\": regions,\n",
        "                    \"actors\": actors,\n",
        "                    \"problem\": matched_keyword,\n",
        "                    \"description\": sent.text,\n",
        "                    \"timeline\": timeline,\n",
        "                    \"conflict_type\": conflict_type\n",
        "                })\n",
        "                break  # Move to the next sentence after finding a match.\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def extract_entities_and_context(doc, taxonomy: dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extracts all relevant structured information from a SpaCy doc:\n",
        "    actors, regions, timeline, conflict type, and problem descriptions.\n",
        "    \"\"\"\n",
        "    # Extract actors.\n",
        "    actors = extractActors(doc, percentile=90)\n",
        "\n",
        "    # Extract region\n",
        "    regions = extractRegion(doc, percentile=90, score_cutoff=85, top_n=5)\n",
        "\n",
        "    # Extract timeline\n",
        "    timeline_matches = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
        "    timeline = next((date for date in timeline_matches if re.search(r\"\\d{4}\", date)), \"Unknown\")\n",
        "    #timeline = extractTimeline(doc)\n",
        "\n",
        "    # Conflict type\n",
        "    conflict_type = extractConflictType(doc,\n",
        "                                        upper_cutoff=95,\n",
        "                                        lower_cutoff=70,\n",
        "                                        step=5,\n",
        "                                        window_size=3)\n",
        "\n",
        "    # Extract problems\n",
        "    problems = extractProblems(doc,\n",
        "                               taxonomy,\n",
        "                               regions,\n",
        "                               actors,\n",
        "                               timeline,\n",
        "                               conflict_type,\n",
        "                               score_cutoff=90)\n",
        "\n",
        "    return problems\n",
        "\n",
        "def saveFiles(data, output_file, file_name, save_csv=False, save_json=False):\n",
        "    \"\"\"\n",
        "    Saves extracted data to CSV and/or JSON.\n",
        "    Appends to existing files and prints a confirmation message.\n",
        "    \"\"\"\n",
        "    if save_csv:\n",
        "      df = pd.DataFrame(data)\n",
        "      df.to_csv(output_file, mode='a', header=False, index=False)\n",
        "      print(f\"✅ Saved to {file_name}\")\n",
        "    if save_json:\n",
        "      with open(output_file, 'a') as json_file:\n",
        "          for entry in data:  # Iterate through the list of dictionaries\n",
        "                  json.dump(entry, json_file, indent=4)\n",
        "                  json_file.write('\\n')\n",
        "      print(f\"✅ Saved to {file_name}\")\n",
        "\n",
        "def analyze_article(url: str, name: str, project_path: str):\n",
        "    \"\"\"\n",
        "    Main pipeline for:\n",
        "    1. Validating and extracting article content.\n",
        "    2. Processing the text with SpaCy.\n",
        "    3. Extracting structured conflict-related data.\n",
        "    4. Saving results to CSV/JSON.\n",
        "    \"\"\"\n",
        "    # Validate the URL\n",
        "    if not validators.url(url):\n",
        "        print(f\"❌ Invalid URL: {url}\")\n",
        "        return\n",
        "\n",
        "    text = extract_text_from_url(url)\n",
        "    if not text.strip():\n",
        "        print(f\"❌ Failed to extract from URL: {url}\")\n",
        "        return\n",
        "\n",
        "    print(\"Starting text processing.\")\n",
        "    doc = nlp(text)\n",
        "    extracted = extract_entities_and_context(doc, taxonomy)\n",
        "\n",
        "    # Adding link to the entry.\n",
        "    for entry in extracted:\n",
        "        entry[\"link\"] = url\n",
        "\n",
        "    if extracted:\n",
        "      print(\"------Found new entries------\")\n",
        "\n",
        "      # Generate timestamped filenames.\n",
        "      timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "      csv_name = f\"{name}_problems_{timestamp}.csv\"\n",
        "      json_name = f\"{name}_problems_{timestamp}.json\"\n",
        "      csv_file = os.path.join(project_path, csv_name)\n",
        "      json_file = os.path.join(project_path, json_name)\n",
        "\n",
        "      # Want to save as .csv?\n",
        "      #input_csv = input(\"Do you want to save the results to a CSV file? (y/n) \")\n",
        "      input_csv = \"y\"\n",
        "      if input_csv.lower().strip() == \"y\":\n",
        "        ensure_file_exists(csv_file)\n",
        "        saveFiles(extracted, csv_file, csv_name, save_csv=True)\n",
        "      else:\n",
        "        print(\"❌ Skipping saving to CSV.\")\n",
        "\n",
        "      # Want to save as .json?\n",
        "      #input_json = input(\"Do you want to save the results to a JSON file? (y/n) \")\n",
        "      input_json = \"y\"\n",
        "      if input_json.lower().strip() == \"y\":\n",
        "        ensure_file_exists(json_file)\n",
        "        saveFiles(extracted, json_file, json_name, save_json=True)\n",
        "      else:\n",
        "        print(\"❌ Skipping saving to JSON.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No new entries found.\")"
      ],
      "metadata": {
        "id": "6LCHHSYjHYwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run code.**"
      ],
      "metadata": {
        "id": "mart5ftDxu38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # What do you want to call this problem compilation?\n",
        "    name = \"GazaIsrael\"\n",
        "\n",
        "    # Where do you want to store your problems?\n",
        "    folder = \"problems1\"\n",
        "\n",
        "    # Which article do you want to analyze? Examples below.\n",
        "    #url = \"https://en.wikipedia.org/wiki/South_Sudanese_Civil_War\"\n",
        "    #url = \"https://en.wikipedia.org/wiki/Russo-Ukrainian_War\"\n",
        "    url = \"https://www.bbc.com/news/articles/cx2vz02e7g8o\"\n",
        "\n",
        "    # Let's analyze!\n",
        "    full_path = os.path.join(full_project_path, folder)\n",
        "    analyze_article(url, name, full_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYzVn6lPxx-L",
        "outputId": "34a4e52f-b561-41cc-9d1a-ee34b39cb5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting text processing.\n",
            "------Found new entries------\n",
            "✅ Created file: /content/drive/MyDrive/Colab Notebooks/Crafting Tech/problems1/GazaIsrael_problems_20250504_150108.csv\n",
            "✅ Saved to GazaIsrael_problems_20250504_150108.csv\n",
            "✅ Created file: /content/drive/MyDrive/Colab Notebooks/Crafting Tech/problems1/GazaIsrael_problems_20250504_150108.json\n",
            "✅ Saved to GazaIsrael_problems_20250504_150108.json\n"
          ]
        }
      ]
    }
  ]
}